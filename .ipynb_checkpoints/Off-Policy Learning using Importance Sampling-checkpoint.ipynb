{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Incremental Update\n",
    "* Off-Policy using importance Sampling\n",
    "* Every visit Monte Carlo Policy Evaluation\n",
    "* Weighted importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All constants and tweakable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discount-Factor\n",
    "GAMMA = 0.9\n",
    "\n",
    "#All possible actions defined\n",
    "ACTION_UP = 'UP'\n",
    "ACTION_DOWN = 'DOWN'\n",
    "ACTION_LEFT = 'LEFT'\n",
    "ACTION_RIGHT = 'RIGHT'\n",
    "\n",
    "#Number of episodes\n",
    "NUMBER_OF_EPISODES = 20\n",
    "\n",
    "#Start and end of any episode\n",
    "START_STATE = '00'\n",
    "END_STATE = '15'\n",
    "\n",
    "#Maximum allowed episode length\n",
    "MAXIMUM_EPISODE_LENGTH = 100\n",
    "\n",
    "#WAIT TIME\n",
    "wait_time = 2\n",
    "\n",
    "#Defining colors for highlighting important aspects\n",
    "GREEN = lambda x: '\\x1b[32m{}\\x1b[0m'.format(x)\n",
    "BLUE = lambda x: '\\x1b[34m{}\\x1b[0m'.format(x)\n",
    "RED = lambda x: '\\x1b[31m{}\\x1b[0m'.format(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datastructures need to traverse and store pertinent information has been defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_states = ['00', '01', '02', '03',\n",
    "          '04', '05', '06', '07',\n",
    "          '08', '09', '10', '11',\n",
    "          '12', '13', '14', '15']\n",
    "\n",
    "immediate_state_rewards = {'00': -1,'01': -1,'02': -1,'03': -1,\n",
    "                           '04': -1,'05': -1,'06': -1,'07': -1,\n",
    "                           '08': -1,'09': -1,'10': -1,'11': -1,\n",
    "                           '12': -1,'13': -1,'14': -1,'15': 0 \n",
    "                          }\n",
    "\n",
    "all_transitions =  {\n",
    "    '00': {ACTION_UP : '00', ACTION_RIGHT : '01', ACTION_DOWN: '04', ACTION_LEFT: '00'},\n",
    "    '01': {ACTION_UP : '01', ACTION_RIGHT : '02', ACTION_DOWN: '05', ACTION_LEFT: '00'},\n",
    "    '02': {ACTION_UP : '02', ACTION_RIGHT : '03', ACTION_DOWN: '06', ACTION_LEFT: '01'},\n",
    "    '03': {ACTION_UP : '03', ACTION_RIGHT : '03', ACTION_DOWN: '07', ACTION_LEFT: '02'},\n",
    "    '04': {ACTION_UP : '00', ACTION_RIGHT : '05', ACTION_DOWN: '08', ACTION_LEFT: '04'},\n",
    "    '05': {ACTION_UP : '01', ACTION_RIGHT : '06', ACTION_DOWN: '09', ACTION_LEFT: '04'},\n",
    "    '06': {ACTION_UP : '02', ACTION_RIGHT : '07', ACTION_DOWN: '10', ACTION_LEFT: '05'},\n",
    "    '07': {ACTION_UP : '03', ACTION_RIGHT : '07', ACTION_DOWN: '11', ACTION_LEFT: '06'},\n",
    "    '08': {ACTION_UP : '04', ACTION_RIGHT : '09', ACTION_DOWN: '12', ACTION_LEFT: '08'},\n",
    "    '09': {ACTION_UP : '05', ACTION_RIGHT : '10', ACTION_DOWN: '13', ACTION_LEFT: '08'},\n",
    "    '10': {ACTION_UP : '06', ACTION_RIGHT : '11', ACTION_DOWN: '14', ACTION_LEFT: '09'},\n",
    "    '11': {ACTION_UP : '07', ACTION_RIGHT : '11', ACTION_DOWN: '15', ACTION_LEFT: '10'},\n",
    "    '12': {ACTION_UP : '08', ACTION_RIGHT : '13', ACTION_DOWN: '12', ACTION_LEFT: '12'},\n",
    "    '13': {ACTION_UP : '09', ACTION_RIGHT : '14', ACTION_DOWN: '13', ACTION_LEFT: '12'},\n",
    "    '14': {ACTION_UP : '10', ACTION_RIGHT : '15', ACTION_DOWN: '14', ACTION_LEFT: '13'},\n",
    "    '15': {ACTION_UP : '15', ACTION_RIGHT : '15', ACTION_DOWN: '15', ACTION_LEFT: '15'},\n",
    "}\n",
    "\n",
    "all_state_action_value_pairs = {\n",
    "    '00': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '01': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '02': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '03': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '04': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '05': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '06': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '07': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '08': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '09': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '10': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '11': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '12': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '13': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '14': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '15': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "}\n",
    "\n",
    "total_state_action_visits = {\n",
    "    '00': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '01': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '02': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '03': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '04': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '05': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '06': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '07': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '08': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '09': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '10': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '11': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '12': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '13': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '14': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '15': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define my target policy as described in the slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_policy =  {\n",
    "    '00': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '01': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '02': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '03': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '04': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '05': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '06': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '07': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '08': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '09': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '10': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '11': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '12': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '13': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '14': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "    '15': {ACTION_UP : 0.1, ACTION_RIGHT : 0.4, ACTION_DOWN: 0.4, ACTION_LEFT: 0.1},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the definition of my behavior policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "behavior_policy =  {\n",
    "    '00': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '01': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '02': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '03': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '04': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '05': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '06': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '07': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '08': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '09': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '10': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '11': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '12': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '13': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '14': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "    '15': {ACTION_UP : 0.25, ACTION_RIGHT : 0.25, ACTION_DOWN: 0.25, ACTION_LEFT: 0.25},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General purpose functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printStateActionValuePairs():\n",
    "    print(\" \\t\", ACTION_UP, \"\\t\", ACTION_RIGHT, \"\\t\", ACTION_DOWN, \"\\t\", ACTION_LEFT)\n",
    "    for state in all_states:\n",
    "        print(state, \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_UP], \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_RIGHT], \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_DOWN], \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_LEFT],)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for generating episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseActionForBehaviorPolicy():\n",
    "    random_throw = random.uniform(0, 1)\n",
    "    if random_throw < 0.25:\n",
    "        return ACTION_UP\n",
    "    elif random_throw < 0.5:\n",
    "        return ACTION_RIGHT\n",
    "    elif random_throw < 0.75:\n",
    "        return ACTION_DOWN\n",
    "    else:\n",
    "        return ACTION_LEFT\n",
    "\n",
    "def generateEpisodeForBehaviorPolicy():\n",
    "    current_state = START_STATE\n",
    "    states_in_episode = []\n",
    "    actions_in_episode = []\n",
    "    while (current_state != END_STATE) & (len(states_in_episode) < MAXIMUM_EPISODE_LENGTH):\n",
    "        states_in_episode.append(current_state)\n",
    "        action = chooseActionForBehaviorPolicy()\n",
    "        actions_in_episode.append(action)\n",
    "        \n",
    "        #os.system('clear')\n",
    "        #printGridWorld(states_in_episode, actions_in_episode)\n",
    "        #time.sleep(wait_time)\n",
    "        \n",
    "        current_state = all_transitions.get(current_state).get(action)\n",
    "        \n",
    "    if current_state == END_STATE:\n",
    "        states_in_episode.append(END_STATE)\n",
    "        actions_in_episode.append(chooseActionForBehaviorPolicy())\n",
    "    \n",
    "    #os.system('clear')\n",
    "    #printGridWorld(states_in_episode, actions_in_episode)\n",
    "    #time.sleep(wait_time)\n",
    "    \n",
    "    return states_in_episode, actions_in_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseActionForTargetPolicy():\n",
    "    random_throw = random.uniform(0, 1)\n",
    "    if random_throw < 0.10:\n",
    "        return ACTION_UP\n",
    "    elif random_throw < 0.5:\n",
    "        return ACTION_RIGHT\n",
    "    elif random_throw < 0.90:\n",
    "        return ACTION_DOWN\n",
    "    else:\n",
    "        return ACTION_LEFT\n",
    "\n",
    "def generateEpisodeForTargetPolicy():\n",
    "    current_state = START_STATE\n",
    "    states_in_episode = []\n",
    "    actions_in_episode = []\n",
    "    while (current_state != END_STATE) & (len(states_in_episode) < MAXIMUM_EPISODE_LENGTH):\n",
    "        states_in_episode.append(current_state)\n",
    "        action = chooseActionForTargetPolicy()\n",
    "        actions_in_episode.append(action)\n",
    "        \n",
    "        #os.system('clear')\n",
    "        #printGridWorld(states_in_episode, actions_in_episode)\n",
    "        #time.sleep(wait_time)\n",
    "        \n",
    "        current_state = all_transitions.get(current_state).get(action)\n",
    "        \n",
    "    if current_state == END_STATE:\n",
    "        states_in_episode.append(END_STATE)\n",
    "        actions_in_episode.append(chooseActionForTargetPolicy())\n",
    "    \n",
    "    #os.system('clear')\n",
    "    #printGridWorld(states_in_episode, actions_in_episode)\n",
    "    #time.sleep(wait_time)\n",
    "    \n",
    "    return states_in_episode, actions_in_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part now implements and runs the algorithm that we have been talking about so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Algorithm followed](images/Algo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t UP \t RIGHT \t DOWN \t LEFT\n",
      "00 \t -7.79 \t -7.66 \t -6.82 \t -7.31\n",
      "01 \t -7.72 \t -7.63 \t -6.92 \t -8.17\n",
      "02 \t -4.96 \t -6.09 \t -4.74 \t -7.11\n",
      "03 \t -6.14 \t -5.51 \t -4.38 \t -5.15\n",
      "04 \t -7.30 \t -6.46 \t -6.60 \t -5.90\n",
      "05 \t -7.18 \t -4.76 \t -6.30 \t -7.23\n",
      "06 \t -6.01 \t -3.37 \t -3.79 \t -9.39\n",
      "07 \t -5.36 \t -5.64 \t -2.85 \t -3.61\n",
      "08 \t -7.93 \t -4.30 \t -6.63 \t -8.28\n",
      "09 \t -5.40 \t -3.32 \t -7.03 \t -4.76\n",
      "10 \t -4.71 \t -3.19 \t -3.21 \t -4.60\n",
      "11 \t -6.90 \t -2.13 \t -1.00 \t -3.60\n",
      "12 \t -8.50 \t -6.23 \t -4.59 \t -7.18\n",
      "13 \t -7.44 \t -5.45 \t -5.31 \t -7.34\n",
      "14 \t -5.28 \t -1.00 \t -3.63 \t -5.40\n",
      "15 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episodes_iterator in range(NUMBER_OF_EPISODES):\n",
    "    states, actions = generateEpisodeForBehaviorPolicy()\n",
    "    returns = 0.0\n",
    "    W = 1\n",
    "    #print(\"Length is \" , str(len(states)))\n",
    "    for step in range(len(states) - 1, -1, -1):\n",
    "        returns = (GAMMA * returns) + immediate_state_rewards[states[step]]\n",
    "        total_state_action_visits[states[step]][actions[step]] = total_state_action_visits[states[step]][actions[step]] + W\n",
    "        all_state_action_value_pairs[states[step]][actions[step]] = all_state_action_value_pairs[states[step]][actions[step]] + (W/total_state_action_visits[states[step]][actions[step]]) * (returns - all_state_action_value_pairs[states[step]][actions[step]])\n",
    "        W = W * ((target_policy[states[step]][actions[step]])/behavior_policy[states[step]][actions[step]])\n",
    "        if W == 0:\n",
    "            break\n",
    "            \n",
    "printStateActionValuePairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
